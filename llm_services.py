# hcc_backend_api/llm_services.py

import os
import io
from PIL import Image
from datetime import datetime
import google.generativeai as genai
from groq import Groq, APIStatusError
from openai import OpenAI

# --- YapÄ±landÄ±rma ---
llm_clients = {}
DOCTOR_LLM_MAP = {
    "Dr. AyÅŸe": "gemini-flash",
    "Dr. Can": "gemini-pro",
    "Dr. Murat": "groq-llama3-70b",
    "Dr. Elif": "groq-llama3-8b",
    "Dr. Zeynep": "deepseek-deepseek-chat",
    "default": "gemini-flash"
}

def load_keys_from_file(filepath='api_keys.txt'):
    """Verilen dosyadan API anahtarlarÄ±nÄ± okur ve bir sÃ¶zlÃ¼k olarak dÃ¶ndÃ¼rÃ¼r."""
    if not os.path.exists(filepath):
        return {}
    keys = {}
    with open(filepath, 'r') as f:
        for line in f:
            if '=' in line:
                key, value = line.strip().split('=', 1)
                keys[key] = value
    return keys

def load_all_llms():
    """TÃ¼m LLM servislerini baÅŸlatÄ±r ve istemcileri yapÄ±landÄ±rÄ±r."""
    global llm_clients
    print("--- LLM Servisleri BaÅŸlatÄ±lÄ±yor ---")

    api_keys = load_keys_from_file()

    # --- Gemini Servisini BaÅŸlat ---
    try:
        GEMINI_API_KEY = api_keys.get("GEMINI_API_KEY")
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            llm_clients['gemini'] = genai
            print("âœ… Gemini Servisi: YapÄ±landÄ±rÄ±ldÄ±")
        else:
            print("âš ï¸ UYARI: api_keys.txt dosyasÄ±nda GEMINI_API_KEY bulunamadÄ±.")
    except Exception as e:
        print(f"HATA: Gemini servisi baÅŸlatÄ±lÄ±rken bir sorun oluÅŸtu: {e}")

    # --- Groq Servisini BaÅŸlat ---
    try:
        GROQ_API_KEY = api_keys.get("GROQ_API_KEY")
        if GROQ_API_KEY:
            llm_clients['groq'] = Groq(api_key=GROQ_API_KEY)
            print("âœ… Groq Servisi: YapÄ±landÄ±rÄ±ldÄ±")
        else:
            print("âš ï¸ UYARI: api_keys.txt dosyasÄ±nda GROQ_API_KEY bulunamadÄ±.")
    except Exception as e:
        print(f"HATA: Groq servisi baÅŸlatÄ±lÄ±rken bir sorun oluÅŸtu: {e}")

# ... (Groq servisinin yapÄ±landÄ±rma bloÄŸu bittikten sonra)

    # --- DeepSeek Servisini BaÅŸlat ---
    try:
        DEEPSEEK_API_KEY = api_keys.get("DEEPSEEK_API_KEY")
        if DEEPSEEK_API_KEY:
            # DeepSeek istemcisi OpenAI kÃ¼tÃ¼phanesi ile yapÄ±landÄ±rÄ±lÄ±r
            llm_clients['deepseek'] = OpenAI(
                api_key=DEEPSEEK_API_KEY,
                base_url="https://api.deepseek.com/v1"
            )
            print("âœ… DeepSeek Servisi: YapÄ±landÄ±rÄ±ldÄ±")
        else:
            print("âš ï¸ UYARI: api_keys.txt dosyasÄ±nda DEEPSEEK_API_KEY bulunamadÄ±.")
    except Exception as e:
        print(f"HATA: DeepSeek servisi baÅŸlatÄ±lÄ±rken bir sorun oluÅŸtu: {e}")

        
# --- DiÄŸer Fonksiyonlar (AynÄ± Kalacak) ---
# get_model_info_for_doctor, generate_radiology_report_vlm, ve 
# generate_comprehensive_report fonksiyonlarÄ±nda hiÃ§bir deÄŸiÅŸiklik yapmanÄ±za gerek yok.
# Onlar olduÄŸu gibi kalabilir.

def get_model_info_for_doctor(doctor_name: str) -> (object, str, str):
    """Doktora gÃ¶re servis istemcisini ve model adÄ±nÄ± dÃ¶ndÃ¼rÃ¼r."""
    print("\n" + "="*40)
    print(f"ğŸ” DOKTOR-MODEL SEÃ‡Ä°MÄ° (Ä°stek ZamanÄ±: {datetime.now().strftime('%H:%M:%S')})")
    
    model_key = DOCTOR_LLM_MAP.get(doctor_name, DOCTOR_LLM_MAP["default"])
    print(f"   - Gelen Doktor AdÄ±: '{doctor_name}'")
    print(f"   - Atanan Model AnahtarÄ±: '{model_key}'")
    
    service_name, model_name = model_key.split('-', 1)
    client = llm_clients.get(service_name)
    
    if client:
        print(f"   - SonuÃ§: '{service_name}' servisinden '{model_name}' modeli kullanÄ±lacak.")
        print("="*40 + "\n")
        return client, model_name, service_name
    else:
        print(f"   - HATA: '{service_name}' servisi yÃ¼klenememiÅŸ. VarsayÄ±lan servise geÃ§iliyor.")
        default_model_key = DOCTOR_LLM_MAP["default"]
        default_service, default_model_name = default_model_key.split('-', 1)
        print("="*40 + "\n")
        return llm_clients.get(default_service), default_model_name, default_service

async def generate_radiology_report_vlm(image_bytes: bytes, predicted_stage_label: str, doctor_name: str):
    """Sadece Gemini gibi VLM destekli modellerle Ã§alÄ±ÅŸÄ±r ve bir sÃ¶zlÃ¼k dÃ¶ndÃ¼rÃ¼r."""
    client, model_name, service_name = get_model_info_for_doctor(doctor_name)
    
    if service_name != 'gemini':
        error_text = f"VLM Raporu oluÅŸturulamadÄ±: SeÃ§ilen doktorun modeli ('{model_name}') gÃ¶rÃ¼ntÃ¼ analizini desteklemiyor. LÃ¼tfen Gemini kullanan bir doktor seÃ§in."
        return {"text": error_text, "model_used": "N/A"}

    if not client:
        return {"text": "VLM Raporu oluÅŸturulamadÄ±: Gemini servisi yÃ¼klenmedi.", "model_used": "N/A"}
    
    model_to_use = 'gemini-1.5-flash-latest' # VLM iÃ§in her zaman flash kullanalÄ±m
    model = client.GenerativeModel(model_to_use)
    prompt_template = f"""AÅŸaÄŸÄ±daki ultrason gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ deÄŸerlendir. TanÄ±: {predicted_stage_label}. Sadece bu gÃ¶rÃ¼ntÃ¼ye gÃ¶re hastalÄ±ÄŸÄ±n mevcut evresine dair tÄ±bbi bir rapor oluÅŸtur. Raporun baÅŸÄ±nda hangi evre olduÄŸu aÃ§Ä±kÃ§a belirtilmeli. Ortalama 4-5 cÃ¼mlelik, profesyonel ve tÄ±bbi bir dille yazÄ±lmÄ±ÅŸ, aÃ§Ä±klayÄ±cÄ± ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir **SONUÃ‡** bÃ¶lÃ¼mÃ¼ Ã¼ret. GiriÅŸ cÃ¼mlesi ya da aÃ§Ä±klama yapma; sadece sonuÃ§ bÃ¶lÃ¼mÃ¼nÃ¼ Ã¼ret."""
    
    try:
        img_for_vlm = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        response = await model.generate_content_async([prompt_template, img_for_vlm])
        report_text = response.text.replace('\n', '<br>').replace('**', '<b>').replace('</b>', '')
        return {"text": report_text, "model_used": model_to_use}
    except Exception as e:
        return {"text": f"Gemini VLM Raporu oluÅŸturma hatasÄ±: {e}", "model_used": model_to_use}


async def generate_comprehensive_report(context: dict, doctor_name: str):
    """Doktor seÃ§imine gÃ¶re Gemini veya Groq kullanarak bÃ¼tÃ¼nsel rapor oluÅŸturur ve sÃ¶zlÃ¼k dÃ¶ndÃ¼rÃ¼r."""
    client, model_name, service_name = get_model_info_for_doctor(doctor_name)
    if not client:
        return {"text": "BÃ¼tÃ¼nsel rapor oluÅŸturulamadÄ±: Uygun LLM servisi yÃ¼klenmemiÅŸ.", "model_used": "N/A"}
        
    prompt = f""" 
# GÃ–REV VE ROL
Sen, hepatoloji ve onkoloji alanlarÄ±nda uzmanlaÅŸmÄ±ÅŸ, farklÄ± tÄ±bbi verileri sentezleyerek kapsamlÄ± bir klinik deÄŸerlendirme raporu hazÄ±rlayan bir yapay zeka asistanÄ±sÄ±n. GÃ¶revin, aÅŸaÄŸÄ±da sunulan verileri analiz ederek bÃ¼tÃ¼ncÃ¼l, yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve profesyonel bir tÄ±bbi rapor oluÅŸturmaktÄ±r.

# HASTA VERÄ°LERÄ°
---------------------------------
**Demografik Bilgiler:**
* **YaÅŸ:** {context.get("patient_details", {}).get('age', 'BelirtilmemiÅŸ')}
* **Cinsiyet:** {context.get("patient_details", {}).get('gender', 'BelirtilmemiÅŸ')}
{f"**Risk FaktÃ¶rleri:** Alkol: {context.get('patient_details', {}).get('alcohol_consumption', 'Bilinmiyor')}, Sigara: {context.get('patient_details', {}).get('smoking_status', 'Bilinmiyor')}"}

**ANALÄ°Z SONUÃ‡LARI**
---------------------------------
**1. Laboratuvar Veri Analizi:** Tahmin: {context.get('lab_result', {}).get('predicted_disease', 'N/A')}, HCC OlasÄ±lÄ±ÄŸÄ±: %{context.get('lab_result', {}).get('hcc_probability', 0) * 100:.2f}
**2. Ultrason (USG) Analizi:** Fibrozis Evresi: {context.get('usg_result', {}).get('stage_label', 'N/A')}
**3. Manyetik Rezonans (MR) Analizi:** NodÃ¼l SayÄ±sÄ±: {context.get('mri_analysis', {}).get('nodule_count', 'N/A')}, TÃ¼mÃ¶r OranÄ±: %{context.get('mri_analysis', {}).get('tumor_ratio', 'N/A')}, Evre: {context.get('mri_analysis', {}).get('stage', 'N/A')}
**4. Klinik Notlar:** {context.get("doctor_note") or "BelirtilmemiÅŸ"}

# Ä°STENEN RAPOR FORMATI
AÅŸaÄŸÄ±daki baÅŸlÄ±klarÄ± kullanarak, yukarÄ±daki verileri sentezleyen detaylÄ± bir rapor oluÅŸtur:
**1. Klinik Ã–zet:**
**2. BulgularÄ±n Entegrasyonu ve YorumlanmasÄ±:**
**3. BÃ¼tÃ¼nsel Risk DeÄŸerlendirmesi:**
**4. Klinik Ã–neri ve Sonraki AdÄ±mlar:**
"""
    
    try:
        if service_name == 'gemini':
            model_to_use = 'gemini-1.5-pro-latest' if model_name == 'pro' else 'gemini-1.5-flash-latest'
            model = client.GenerativeModel(model_to_use)
            response = await model.generate_content_async(prompt)
            return {"text": response.text, "model_used": model_to_use}
            
        elif service_name == 'groq':
            model_to_use = 'llama3-70b-8192' if model_name == 'llama3-70b' else 'llama3-8b-8192'
            chat_completion = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=model_to_use,
            )
            return {"text": chat_completion.choices[0].message.content, "model_used": model_to_use}

        elif service_name == 'deepseek':
            # model_name deÄŸiÅŸkeni DOCTOR_LLM_MAP'ten 'deepseek-chat' olarak gelir
            chat_completion = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=model_name,
            )
            return {"text": chat_completion.choices[0].message.content, "model_used": model_name}


    except APIStatusError as e:
        return {"text": f"API HatasÄ± ({service_name}): {e.status_code} - {e.message}", "model_used": "Hata"}
    except Exception as e:
        return {"text": f"Rapor oluÅŸturma hatasÄ± ({service_name}): {e}", "model_used": "Hata"} 